{
    "contents" : "Regression and Variable Selection\n========================================================\n\nThe objective of this case is to get you started with regression model building, variable selection, and model evaluation in R, and help you with Case 2.\n\nCode in this file is not the only correct way to do things, however it is important for you to understand what each statement does. You will have to modify the code accordingly for your homework. \n\nBoston Housing Data\n-------\n### Load Data\n```{r}\nlibrary(MASS)\ndata(Boston); #this data is in MASS package\ncolnames(Boston) \n```\nThe original data are 506 observations on 14 variables, medv being the response variable $y$:\n\nVariable |Description\n---------|-----\ncrim | per capita crime rate by town\nzn\t |proportion of residential land zoned for lots over 25,000 sq.ft\nindus\t| proportion of non-retail business acres per town\nchas\t| Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\nnox\t| nitric oxides concentration (parts per 10 million)\nrm\t| average number of rooms per dwelling\nage\t| proportion of owner-occupied units built prior to 1940\ndis\t| weighted distances to five Boston employment centres\nrad\t| index of accessibility to radial highways\ntax\t| full-value property-tax rate per USD 10,000\nptratio\t| pupil-teacher ratio by town\nblack\t|1000(B - 0.63)^2 where B is the proportion of blacks by town\nlstat\t| percentage of lower status of the population\nmedv\t| median value of owner-occupied homes in USD 1000's\n\n### Sampling (Split Dataset Randomly)\nNext we sample 90% of the original data and use it as the training set. The remaining 10% is used as test set. The regression model will be built on the training set and future performance of your model will be evaluated with the test set.\n\n```{r}\nsubset <- sample(nrow(Boston),nrow(Boston)*0.90)\nBoston_train = Boston[subset,]\nBoston_test = Boston[-subset,]\n```\nNote: The random sample will change in each run. If you wish to get the same sample each time you run the line of code, add *set.seed(###)* before each random statement, where *###* is a large integer number. For example,\n```{r}\nset.seed(12345)\nsubset <- sample(nrow(Boston),nrow(Boston)*0.90)\nBoston_train = Boston[subset,]\nBoston_test = Boston[-subset,]\n```\nthen each time you run the code, you will get the same random sample. This will be particularly helpful when you are debugging the code.\n\n### (Optional) Standardization\nIf we want our results to be invariant to the units and the parameter estimates $\\beta_i$ to be comparable, we can standardize the variables. Essentially we are replacing the original values with their z-scores.\n\n1st Way: create new variables manually.\n```{r, eval=FALSE}\nBoston$sd.crim<-(Boston$crim-mean(Boston$crim))/sd(Boston$crim); \n```\n\nThis does the same thing.\n```{r,eval=FALSE}\nBoston$sd.crim<- scale(Boston$crim); \n```\n\n\n2nd way: If you have a lot of variables to standardize then the above is not very pleasing to do. You can use a loop like this. It standardizes every varables other than the last one which is $y$.\n\n```{r}\nfor (i in 1:(ncol(Boston_train)-1)){\n  Boston_train[,i] = scale(Boston_train[,i])\n}\n```\n\nThe technique is not as important in linear regression because it will only affect the interpretation but not the model estimation and inference. \n\nModel Building\n------------------\nYou task is to build a best model with training data. You can refer to the regression and variable selection code on the slides for more detailed description of linear regression.\n\nThe following model includes all $x$ varables in the model\n```{r, eval=FALSE}\nmodel_1 <- lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat, data=Boston_train)\n```\n\nTo include all variables in the model, you can write the statement this simpler way.\n\n```{r}\nmodel_1 <- lm(medv~., data=Boston_train)\nsummary(model_1)\n```\n\nBut, is this the model you want to use?\n\n### (Optional) Interaction terms in model\nIf you suspect the effect of one predictor x1 on the response y depends on the value of another predictor x2, you can add interaction terms in model. To specify interaction in the model, you put : between two variables with interaction effect. For example\n```{r, eval=FALSE}\nlm(medv~crim+zn+crim:zn, data=Boston_train)\n#The following way automatically add the main effects of crim and zn\nlm(medv~crim*zn, data=Boston_train)\n```\nFor now we will not investigate the interactions of variables.\n\nEvaluating Model Fitness \n------------\n### In-sample model evaluation (train error)\nMSE of the regression, which is the square of 'Residual standard error' in the above summary. It is the sum of squared residuals(SSE) divided by degrees of freedom (n-p-1). In some textbooks the sum of squred residuals(SSE) is called residual sum of squares(RSS). MSE of the regression should be the unbiased estimator for variance of $\\epsilon$, the error term in the regression model.\n\n```{r}\nmodel_summary <- summary(model_1)\n(model_summary$sigma)^2\n```\n\n$R^2$ of the model\n```{r}\nmodel_summary$r.squared\n```\n\nAdjusted-$R^2$ of the model, if you add a variable (or several in a group), SSE will decrease, $R^2$ will increase, but Adjusted-$R^2$ could go either way.\n```{r}\nmodel_summary$adj.r.squared\n```\n\nAIC and BIC of the model, these are information criteria. Smaller values indicate better fit.\n\n```{r}\nAIC(model_1)\nBIC(model_1)\n```\n\nBIC, AIC, and Adjusted $R^2$ have complexity penalty in the definition, thus when comparing across different models they are better indicators on how well the model will perform on future data.\n\n### Out-of-sample prediction (test error)\nTo evaluate how the model performs on future data, we use predict() to get the predicted values from the test set.\n```{r, eval=FALSE}\n#pi is a vector that contains predicted values for test set.\npi <- predict(object = model_1, newdata = Boston_test)\n```\nJust as any other function, you can write the above statement the following way as long as the arguments are in the right order.\n\n```{r, echo=FALSE}\nsubset <- sample(nrow(Boston),nrow(Boston)*0.90)\nBoston_train = Boston[subset,]\nBoston_test = Boston[-subset,]\nmodel_1 <- lm(medv~., data=Boston_train)\n```\n\n```{r, eval=TRUE}\npi <- predict(model_1, Boston_test)\n```\n\nThe most common measure is the Mean Squared Error (MSE): average of the squared differences between the predicted and actual values\n```{r}\nmean((pi - Boston_test$medv)^2)\n```\nA less popular measure is the Mean Absolute Error (MAE). You can probably guess that here instead of taking the average of squared error, MAE is the average of absolute value of error.\n```{r}\nmean(abs(pi - Boston_test$medv))\n```\n\nNote that if you ignore the second argument of predict(), it gives you the in-sample prediction on the training set:\n```{r, eval=FALSE}\npredict(model_1)\n```\nWhich is the same as\n```{r, eval=FALSE}\nmodel_1$fitted.values\n```\n\n\nVariable Selection\n------------------------\n### Compare Model Fit Manually\n```{r eval=FALSE}\nmodel_1 <- lm(medv~., data = Boston_train)\nmodel_2 <- lm(medv~crim+zn, data = Boston_train)\nsummary(model_1)\nsummary(model_2)\nAIC(model_1)\nAIC(model_2)\n```\n\n### Best Subset Regression\nThe 'leaps' package provides procedures for best subset regression.\n```{r eval=FALSE}\ninstall.packages('leaps')\n```\n```{r, warning=FALSE}\nlibrary(leaps)\n```\nWhich subset of variables should you include in order to minimize BIC?\n```{r}\n#regsubsets only takes data frame as input\nsubset_result <- regsubsets(medv~.,data=Boston_train, nbest=2, nvmax = 14)\nsummary(subset_result)\nplot(subset_result, scale=\"bic\")\n```\nEach row represents a model. Black indicates that a variable is included in the model, while white indicates that it is not. \nThe scale = \"\" can be \"Cp\", \"adjr2\", \"r2\" or \"bic\".\n\nWhat is the problem with best subset regression? If there are n independent variables, the number of possible nonempty subsets is 2^n - 1. If you try a best subset regression with more than 50 variables, you might need to wait for your entire life to get the result.\n\n<img src=\"http://science.slc.edu/~jmarshall/courses/2002/spring/cs50/BigO/running-times.gif\" height=\"300px\" />\n\n### Forward/Backward/Stepwise Regression Using AIC\nTo perform the Forward/Backward/Stepwise Regression in R, we need to define the starting points:\n```{r}\nnullmodel=lm(medv~1, data=Boston_train)\nfullmodel=lm(medv~., data=Boston_train)\n```\nnullmodel is the model with no varaible in it, while fullmodel is the model with every variable in it.\n\n#### Backward Elimination\n```{r}\nmodel.step<-step(fullmodel,direction='backward')\n```\n\n#### Forward Selection\n```{r}\nmodel.step<-step(nullmodel, scope=list(lower=nullmodel, upper=fullmodel), direction='forward')\n```\n#### Stepwise Selection (Output Omitted)\n```{r, eval=FALSE}\nmodel.step<-step(nullmodel, scope=list(lower=nullmodel, upper=fullmodel), direction='both')\n```\n\nOne caution when comparing fit statistics using AIC, the definition varies by program/procedure.\n```{r}\nextractAIC(model_1)\nAIC(model_1)\n```\n\n* For pros and cons of variable/model selection using the common fit statistics: (adjusted) $R^2$, MSE, AIC, BIC, etc. refer to Ch9 in \"Applied Linear Regression Models\" by Kutner et al.\n* For other variable selection methods refer to section 3.4 - 3.8 of [\"Elements of Statistical Learning\" (Free Online)](http://www-stat.stanford.edu/~tibs/ElemStatLearn/).\n\nCross Validation\n-----------------\nCross validation is an alternative approach to training/testing split. For k-fold cross validation, the dataset is divided into k parts. Each part serves as the test set in each iteration and the rest serve as training set. The out-of-sample performance measures from the k iterations are averaged.\n\nNote\n\n1. We use the **entire** dataset for cross validation\n\n2. We need to use glm instead of lm to fit the model (if we want to use cv.glm fucntion in boot package)\n\n3. The default measure of performance is the Mean Squared Error (MSE). If we want to use another measure we need to define a cost function.\n\n### 5-fold Cross Validation\n```{r}\nlibrary(boot)\nmodel_2 = glm(medv~indus + rm, data = Boston)\ncv.glm(data = Boston, glmfit = model_2, K = 5)$delta[2]\n```\n### LOOCV (Leave-one-out Cross Validation)\n```{r}\ncv.glm(data = Boston, glmfit = model_2, K = nrow(Boston))$delta[2]\n```\n### 5-fold Cross Validation Using MAE\nHere we need to define a MAE cost function. The function takes 2 input vectors, pi =  predicted values, r = actual values.\n\n```{r}\nmodel_2 = glm(medv~indus + rm, data = Boston)\n\nMAE_cost = function(pi, r){\n  return(mean(abs(pi-r)))\n}\n\ncv.glm(data = Boston, glmfit = model_2, cost = MAE_cost, K = 5)$delta[2]\n```\n\n\nAnother package DAAG also does cross validation. It prints out the performance in each fold and gives you a plot at the end. But currently I cannot figure out how to get the cross-validation error programmatically.\n\n```{r, eval=FALSE}\ninstall.packages('DAAG')\n```\n\n```{r,message=FALSE, eval=FALSE}\nlibrary(DAAG)\n```\n```{r, warning=FALSE, eval=FALSE}\nmodel_2 = lm(medv~indus + rm, data = Boston)\ncv.lm(df=Boston, form.lm = model_2, m=3)\n```\n\n\n\n\n\nDiagnostic Plots\n-----------------\nThe diagnostic plots are not as important when regression is used in predictive (supervised) data mining as when it is used in economics. However it is still good to know:\n\n1. What the diagnostic plots should look like when no assumption is violated?\n\n2. If there is something wrong, what assumptions are possibly violated?\n\n3. What implications does it have on the analysis?\n\n4. (How) can I fix it?\n\nRoughly speaking, the table summarizes what you should look for in the following plots\n\nPlot Name  | Good  \n------------- | -------------\nResidual vs. Fitted  | No pattern, scattered around 0 line\nNormal Q-Q | Dots fall on dashed line \nResidual vs. Leverage | No observation with large Cook's distance\n\n```{r}\nplot(model_1)\n```\n",
    "created" : 1417292840183.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "470089021",
    "id" : "E66C2DE5",
    "lastKnownWriteTime" : 1417292838,
    "path" : "C:/Users/Young Comp/Downloads/Case 1 Lab-Sample Rmd.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}